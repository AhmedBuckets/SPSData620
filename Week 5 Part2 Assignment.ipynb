{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9778c41c",
   "metadata": {},
   "source": [
    "# Week 5 Part 2\n",
    "Benson Toi, Noah Collin, Ahmed Elsaeyed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c842f51",
   "metadata": {},
   "source": [
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  UCI Machine Learning Repository: Spambase Data Set\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n",
    "This assignment is due end of day on Sunday.\n",
    "\n",
    "The dataset we will be using is available here: https://archive.ics.uci.edu/ml/datasets/spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b41c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19943ce",
   "metadata": {},
   "source": [
    "# Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f837fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44145b9",
   "metadata": {},
   "source": [
    "Without headers we get a generic enumeration of the column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fb151e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcf6a2",
   "metadata": {},
   "source": [
    "The website also provides us with the actual column names which we can add in thanks to pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ac1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['word_freq_make','word_freq_address','word_freq_all','word_freq_3d','word_freq_our','word_freq_over','word_freq_remove','word_freq_internet','word_freq_order','word_freq_mail','word_freq_receive','word_freq_will','word_freq_people','word_freq_report','word_freq_addresses','word_freq_free','word_freq_business','word_freq_email','word_freq_you','word_freq_credit','word_freq_your','word_freq_font','word_freq_000','word_freq_money','word_freq_hp','word_freq_hpl','word_freq_george','word_freq_650','word_freq_lab','word_freq_labs','word_freq_telnet','word_freq_857','word_freq_data','word_freq_415','word_freq_85','word_freq_technology','word_freq_1999','word_freq_parts','word_freq_pm','word_freq_direct','word_freq_cs','word_freq_meeting','word_freq_original','word_freq_project','word_freq_re','word_freq_edu','word_freq_table','word_freq_conference','char_freq_;','char_freq_(','char_freq_[','char_freq_!','char_freq_$','char_freq_#','capital_run_length_average','capital_run_length_longest','capital_run_length_total','is_spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e45bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  is_spam  \n",
       "0                       278        1  \n",
       "1                      1028        1  \n",
       "2                      2259        1  \n",
       "3                       191        1  \n",
       "4                       191        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80869d",
   "metadata": {},
   "source": [
    "# Splitting the Data into Testing and Training\n",
    "\n",
    "Some \"natural language processing\" has been done to an extent based on the columns above. Our model can be created in sklearn such that the first 57 columns are features, and the last one is the label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c1249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df named X that represents only the first 57 columns\n",
    "X = df.iloc[:,:-1]\n",
    "\n",
    "# create a df named y that represents only the last column\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a383777",
   "metadata": {},
   "source": [
    "Sklearn lets us easily create testing and training data sets from an exisiting data set using the function train_test_split :https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51dccc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size= 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a679aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3220"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the lengths of the dfs we just made to make sure: \n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a6c497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e3364b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4601"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the sum of the above should equal the length of the original df\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e9c3d",
   "metadata": {},
   "source": [
    "# Initial Model Creation - Bernoulli Naive Bayes\n",
    "\n",
    "The first step is to create a model object using the type of model we want to make. We use the Bernoulli naive bayes at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe7595c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1]\n",
      "[0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "bernoulli_nb_classifier = BernoulliNB()\n",
    "# The next step is to train the model using the training data we extracted before\n",
    "# The function 'fit' takes the feature set and the labels as inputs and 'fits' the features to the labels\n",
    "bernoulli_nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# How to get a prediciton\n",
    "# The BernoulliNB() object has a function 'predict' which takes in a single row of features and returns a predicted label\n",
    "# Here we can use any random row from our 'X_test' data\n",
    "bernoulli_nb_classifier.predict(X_test.iloc[[56]])\n",
    "bernoulli_nb_classifier.predict(X_test.iloc[[60]])\n",
    "bernoulli_nb_classifier.predict(X_test.iloc[[1000]])\n",
    "\n",
    "\n",
    "# We can also pass in several rows of data from X_test and get predictions for all of them \n",
    "print(bernoulli_nb_classifier.predict(X_test[0:30]))\n",
    "\n",
    "# We can then check the accuracy of the model against the dataset y_test, which contains the actual labels\n",
    "print(y_test[0:30].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd6bd4",
   "metadata": {},
   "source": [
    "# Exploring Different Models\n",
    "\n",
    "As we can see the accuracy of the predictions is not that good. We can repeat this with other classifiers to see if we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a6a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1]\n",
      "[0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Trying it again with another classifier, Gaussian Naive Bayes\n",
    "\n",
    "gnbc = GaussianNB()\n",
    "gnbc.fit(X_train, y_train)\n",
    "print(gnbc.predict(X_test[0:30]))\n",
    "print(y_test[0:30].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b8600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1]\n",
      "[0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Trying it again with Multinomial Naive Bayes\n",
    "\n",
    "mnbc = MultinomialNB()\n",
    "mnbc.fit(X_train, y_train)\n",
    "print(mnbc.predict(X_test[0:30]))\n",
    "print(y_test[0:30].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120638a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1]\n",
      "[0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Trying it again with Categorical Naive Bayes\n",
    "\n",
    "cnbc = CategoricalNB()\n",
    "cnbc.fit(X_train, y_train)\n",
    "print(cnbc.predict(X_test[0:30]))\n",
    "print(y_test[0:30].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22624f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1]\n",
      "[0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Trying it again with linear regression\n",
    "\n",
    "lrc = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "lrc.fit(X_train, y_train)\n",
    "print(lrc.predict(X_test[0:30]))\n",
    "print(y_test[0:30].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ab7d5",
   "metadata": {},
   "source": [
    "It looks like linear regression gives us the best predictions. These scores can be improved of course with more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aabe00",
   "metadata": {},
   "source": [
    "# A More Scientific Way of Measuring Model Accuracy\n",
    "\n",
    "The function roc_auc_score is a function is scikitlearn that does area-under-curve analysis of a model's output.\n",
    "The higher the number, the higher the ratio of true positives to false positives. We can see that linear regression provides the best model so far, followed by categorical naive bayes.\n",
    "\n",
    "More about area-under-the-curve and how its used to evaluate models is available here: \n",
    "https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/#:~:text=The%20Area%20Under%20the%20Curve,the%20positive%20and%20negative%20classes.\n",
    "\n",
    "From the link: \"When AUC = 1, then the classifier is able to perfectly distinguish between all the Positive and the Negative class points correctly. If, however, the AUC had been 0, then the classifier would be predicting all Negatives as Positives, and all Positives as Negatives.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d73c52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9747468012710094\n",
      "0.9733912758564197\n"
     ]
    }
   ],
   "source": [
    "# score for linear regression model\n",
    "print(roc_auc_score(y_train, lrc.predict_proba(X_train)[:,1], multi_class='ovr') )\n",
    "\n",
    "# score using the test data set\n",
    "print(roc_auc_score(y_test, lrc.predict_proba(X_test)[:,1], multi_class='ovr') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ef11d",
   "metadata": {},
   "source": [
    "We see that for linear regression, it does a little worse on the test data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66412380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9669575454770853\n"
     ]
    }
   ],
   "source": [
    "# score for categorical naive bayes\n",
    "print(roc_auc_score(y_train, cnbc.predict_proba(X_train)[:,1], multi_class='ovr') )\n",
    "\n",
    "# score using the test data set throws an erorr for some reason\n",
    "#print(roc_auc_score(y_test, cnbc.predict_proba(X_test)[:,1], multi_class='ovr') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73bdc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9509557446628041\n",
      "0.9561928227148486\n"
     ]
    }
   ],
   "source": [
    "# score for bernoulli naive bayes\n",
    "print(roc_auc_score(y_train, bernoulli_nb_classifier.predict_proba(X_train)[:,1], multi_class='ovr') )\n",
    "print(roc_auc_score(y_test, bernoulli_nb_classifier.predict_proba(X_test)[:,1], multi_class='ovr') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8c59f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9453477741022027\n",
      "0.948221414590824\n"
     ]
    }
   ],
   "source": [
    "# score for gaussian naive bayes\n",
    "print(roc_auc_score(y_train, gnbc.predict_proba(X_train)[:,1], multi_class='ovr') )\n",
    "print(roc_auc_score(y_test, gnbc.predict_proba(X_test)[:,1], multi_class='ovr') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c78e2fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8474456248695063\n",
      "0.8553129068694655\n"
     ]
    }
   ],
   "source": [
    "# score for multinomial naive bayes\n",
    "print(roc_auc_score(y_train, mnbc.predict_proba(X_train)[:,1], multi_class='ovr') )\n",
    "print(roc_auc_score(y_test, mnbc.predict_proba(X_test)[:,1], multi_class='ovr') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fdfe81",
   "metadata": {},
   "source": [
    "The naive Bayes models do a little bit better with the test dataset, but not a significant amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81994c85",
   "metadata": {},
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070f3c7",
   "metadata": {},
   "source": [
    "Precision is another function provided by scikitlearn, documented here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "From the documentation: \"The precision is intuitively the ability of the classifier not to label as positive a sample that is negative\". \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "698d3586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321089229209281\n"
     ]
    }
   ],
   "source": [
    "y_pred = lrc.predict(X_test)\n",
    "print(precision_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78a02736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8362268003677202\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = gnbc.predict(X_test)\n",
    "print(precision_score(y_test, y_pred2, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d129c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
